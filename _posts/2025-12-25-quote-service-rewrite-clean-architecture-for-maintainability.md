---
layout: single
title: "Quote Service Rewrite: Clean Architecture for Long-Term Maintainability"
date: 2025-12-25
permalink: /posts/2025/12/quote-service-rewrite-clean-architecture-for-maintainability/
categories:
  - blog
tags:
  - solana
  - go
  - architecture
  - clean-architecture
  - refactoring
  - design-patterns
  - microservices
  - hft
  - trading
  - christmas
excerpt: "ğŸ„ Merry Christmas! Planning a comprehensive rewrite of quote-service with clean architecture principles: 85% code reduction, 20x faster quotes, 4x better test coverage, and dramatically improved maintainability. Why we're rebuilding the foundation for long-term success."
---

## ğŸ„ Merry Christmas and Happy New Year! ğŸ„

On this Christmas Day 2025, I'm taking a moment to reflect on the journey of building this Solana HFT trading system. As we celebrate with family and friends, I'm also planning the next major evolution of our architecture.

**Wishing everyone a Merry Christmas and a prosperous Happy New Year!** May 2026 bring successful trades, robust systems, and minimal bugs! ğŸ‰

Today's post is a bit differentâ€”instead of implementation details, I'm sharing the **architectural rewrite plan** for our quote-service. It's a story of **technical debt, lessons learned, and the path to sustainable architecture**.

---

## TL;DR

Planning a comprehensive rewrite of quote-service with clean architecture principles:

1. **85% Code Reduction**: 50K lines â†’ 15K lines through proper separation of concerns
2. **20x Faster Quotes**: 200ms â†’ 10ms by separating pool discovery from quote serving
3. **4x Better Test Coverage**: 20% â†’ 80%+ with dependency injection and interfaces
4. **Dramatically Better Maintainability**: Internal packages, clean architecture, single responsibility
5. **Service Separation**: 3 services (quote, pool discovery, RPC proxy) vs 1 monolith
6. **Technology Decision**: Go for speed (2-3 weeks), Rust RPC proxy for shared infrastructure

**The Core Insight**: The current quote-service **works**, but it's **unmaintainable**. We need to rebuild the foundation now before technical debt makes future changes impossible.

---

## Table of Contents

1. [The Problem: Why Rewrite a Working System?](#the-problem-why-rewrite-a-working-system)
2. [Current Architecture: Design Flaws](#current-architecture-design-flaws)
3. [New Architecture: Clean Separation](#new-architecture-clean-separation)
4. [Go vs Rust Decision](#go-vs-rust-decision)
5. [HTTP + gRPC: Combined vs Split](#http--grpc-combined-vs-split)
6. [Clean Architecture Benefits](#clean-architecture-benefits)
7. [Technology Stack Decisions](#technology-stack-decisions)
8. [Expected Improvements](#expected-improvements)
9. [Conclusion: Building for the Future](#conclusion-building-for-the-future)

---

## The Problem: Why Rewrite a Working System?

### It Works, But...

The current quote-service is **feature-complete and functional**:
- âœ… Serves quotes via HTTP and gRPC
- âœ… Supports 6 DEX protocols (Raydium, Meteora, Orca, Pump.fun)
- âœ… Real-time WebSocket updates
- âœ… 99.99% availability with RPC pool
- âœ… Redis crash recovery
- âœ… Full observability (Grafana LGTM stack: Loki, Grafana, Tempo, Mimir)

**So why rewrite?**

Because **"works" is not enough for long-term success**. The system has critical architectural flaws that make it:
1. **Difficult to maintain** - 96KB `cache.go` file with 50+ methods
2. **Hard to test** - Tightly coupled components, 20% test coverage
3. **Slow to extend** - Adding features requires touching multiple files
4. **Risky to deploy** - No confidence in changes due to poor testing
5. **Impossible to reason about** - Mixed concerns everywhere

### The Technical Debt Reality

```
Current Codebase Health:
â”œâ”€â”€ Lines of Code: 50,000+ (monolithic)
â”œâ”€â”€ Test Coverage: ~20% (hard to test)
â”œâ”€â”€ Files in cmd/: 20+ files (violates Go standards)
â”œâ”€â”€ Largest File: 96KB cache.go (unmaintainable)
â””â”€â”€ Architectural Pattern: Big Ball of Mud âŒ
```

**This is a ticking time bomb.** Every feature we add makes it worse. Every bug fix becomes harder. Eventually, we'll reach a point where the system is **too complex to understand** and **too risky to change**.

**The time to fix this is NOW, while we still can.**

---

## Current Architecture: Design Flaws

### Flaw #1: Monolithic `cache.go` (96KB, 50+ methods)

**The Problem:**

```go
// cache.go mixes EVERYTHING in one file:
type QuoteCache struct {
    router            *pkg.SimpleRouter      // Pool routing
    solClient         *sol.Client            // RPC client âŒ
    wsPool            *subscription.WSPool   // WebSocket âŒ
    oraclePriceFetcher *oracle.PriceFetcher  // Oracle
    cache             map[string]*CachedQuote // Actual cache
    poolLiquidity     map[string]float64     // Pool state âŒ
    // ... 20 more fields
}

// 50+ methods that do everything:
func (c *QuoteCache) UpdateQuote()          // Quote refresh
func (c *QuoteCache) DiscoverPools()        // Pool discovery âŒ
func (c *QuoteCache) ManageRPCPool()        // RPC management âŒ
func (c *QuoteCache) HandleWebSocket()      // WebSocket âŒ
// ... 46 more methods
```

**Why This Is Bad:**
- **Violates Single Responsibility Principle** - Does 5 different things
- **Impossible to test in isolation** - Too many dependencies
- **Cannot reason about code** - 96KB file is too large to hold in your head
- **Changes have unpredictable side effects** - Everything is interconnected

**What Should Happen:**
- `QuoteCache` should ONLY cache quotes (1 responsibility)
- Pool discovery â†’ Separate service
- RPC management â†’ Rust RPC Proxy
- WebSocket â†’ Pool discovery service

### Flaw #2: RPC Logic Embedded in Service

**The Problem:**

```
pkg/sol/rpc_pool.go (1200+ lines)
â”œâ”€â”€ RPC pool management
â”œâ”€â”€ Health monitoring
â”œâ”€â”€ Rate limiting
â”œâ”€â”€ Failover logic
â””â”€â”€ Cannot be reused by other services âŒ
```

**Why This Is Bad:**
- **Code duplication** - Scanner needs RPC pool, must copy-paste
- **Inconsistent behavior** - Each service implements RPC differently
- **Wasted effort** - Solving the same problem multiple times
- **Bugs multiply** - Fix a bug in quote-service, scanner still broken

**What Should Happen:**
- Centralized Rust RPC Proxy (see [docs/25-RUST-RPC-PROXY-DESIGN.md](https://github.com/guidebee/solana-trading-system))
- Used by ALL services (quote, scanner, executor)
- Single source of truth for RPC management

### Flaw #3: Pool Discovery During Quote Serving

**The Problem:**

```
Every 30 seconds:
1. UpdateQuote() triggered
2. For each pair:
   â”œâ”€ QueryAllPools() â† Makes RPC calls! âŒ
   â”œâ”€ Fetch pool state from blockchain (200ms)
   â”œâ”€ Calculate quote
   â””â”€ Cache result

PROBLEM: Discovery blocks quote serving!
```

**Why This Is Bad:**
- **Slow** - Discovery takes 200ms, blocks quote serving
- **Unreliable** - RPC failures cause quote serving to fail
- **Wasteful** - Discovering same pools every 30s
- **Tight coupling** - Quote logic mixed with discovery logic

**What Should Happen:**
- Separate pool-discovery-service (runs every 5 minutes)
- Writes discovered pools to Redis
- Quote-service just reads from Redis (0.5ms)
- **No blocking, no coupling**

### Flaw #4: No Internal Packages

**The Problem:**

```
Current (WRONG):
go/cmd/quote-service/
â”œâ”€â”€ main.go
â”œâ”€â”€ cache.go
â”œâ”€â”€ grpc_server.go
â”œâ”€â”€ handler_*.go (10 files)
â””â”€â”€ ... all logic in cmd/ âŒ

Problems:
- Violates Go project layout standards
- Cannot import logic in other services
- Difficult to test (no interfaces)
- Everything is tightly coupled
```

**What Should Happen:**

```
Correct Structure:
go/
â”œâ”€â”€ cmd/quote-service/
â”‚   â””â”€â”€ main.go (ONLY DI wiring, 100 lines)
â”‚
â””â”€â”€ internal/quote-service/
    â”œâ”€â”€ domain/       # Interfaces + models
    â”œâ”€â”€ repository/   # Data access (Redis, cache)
    â”œâ”€â”€ calculator/   # Quote calculation
    â”œâ”€â”€ service/      # Business logic
    â””â”€â”€ api/          # HTTP + gRPC handlers
```

**Benefits:**
- âœ… Clean separation of concerns
- âœ… Easy to test (inject mocks via interfaces)
- âœ… Each package has ONE responsibility
- âœ… Follows Go best practices

### Flaw #5: Hard to Test

**Current Test Coverage: 20%** âŒ

**Why So Low?**

```go
// Current code (impossible to test):
func (c *QuoteCache) UpdateQuote() {
    // Hard-coded RPC client âŒ
    pools := c.solClient.QueryAllPools(...)

    // Hard-coded WebSocket âŒ
    c.wsPool.Subscribe(...)

    // No interfaces, cannot inject mocks âŒ
}

// To test this, you need:
- Real RPC endpoint (flaky, slow)
- Real WebSocket connection (flaky, slow)
- Real Redis (integration test, not unit test)
- Full infrastructure (NATS, Prometheus, etc.)

Result: Nobody writes tests, coverage stays at 20%
```

**What Should Happen:**

```go
// New code (easy to test):
type QuoteService struct {
    poolRepo      domain.PoolReader      // Interface! âœ…
    calculator    domain.PriceCalculator // Interface! âœ…
    cacheManager  domain.CacheManager    // Interface! âœ…
}

// To test this:
func TestQuoteService(t *testing.T) {
    // Inject mocks! No real infrastructure needed!
    mockPoolRepo := &MockPoolReader{}
    mockCalculator := &MockPriceCalculator{}
    mockCache := &MockCacheManager{}

    service := NewQuoteService(mockPoolRepo, mockCalculator, mockCache)

    // Test business logic in isolation âœ…
    quote, err := service.GetQuote(ctx, "SOL", "USDC", 1000000000)
    assert.NoError(t, err)
    assert.Equal(t, expectedOutput, quote.OutputAmount)
}

Result: 80%+ test coverage, fast unit tests âœ…
```

---

## New Architecture: Clean Separation

### High-Level Architecture

**Before (Monolithic):**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Quote Service (Single Monolith)          â”‚
â”‚                                                   â”‚
â”‚  â€¢ Quote caching     (Good âœ…)                    â”‚
â”‚  â€¢ Pool discovery    (Blocks serving âŒ)          â”‚
â”‚  â€¢ RPC management    (Should be shared âŒ)        â”‚
â”‚  â€¢ WebSocket updates (Blocks serving âŒ)          â”‚
â”‚  â€¢ HTTP API          (Good âœ…)                    â”‚
â”‚  â€¢ gRPC streaming    (Good âœ…)                    â”‚
â”‚                                                   â”‚
â”‚  PROBLEMS:                                        â”‚
â”‚  - 50K lines, unmaintainable                      â”‚
â”‚  - Discovery blocks quote serving                 â”‚
â”‚  - RPC logic cannot be reused                     â”‚
â”‚  - Hard to test (20% coverage)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**After (Clean Separation):**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Pool Discovery Service (NEW - Independent)     â”‚
â”‚  â€¢ Discovers pools every 5 minutes                  â”‚
â”‚  â€¢ Writes to Redis (pool metadata)                  â”‚
â”‚  â€¢ No blocking, runs in background                  â”‚
â”‚  â€¢ 8K lines, single responsibility âœ…               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“ Redis
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Quote Service (REWRITTEN - Clean Architecture)  â”‚
â”‚  â€¢ Reads pools from Redis (fast, no blocking) âœ…    â”‚
â”‚  â€¢ ONLY quote calculation + serving âœ…              â”‚
â”‚  â€¢ HTTP + gRPC in one service (shared cache) âœ…     â”‚
â”‚  â€¢ 15K lines, clean architecture âœ…                 â”‚
â”‚  â€¢ 80%+ test coverage âœ…                            â”‚
â”‚                                                     â”‚
â”‚  Internal Structure:                                â”‚
â”‚  â”œâ”€â”€ domain/      (interfaces, models)              â”‚
â”‚  â”œâ”€â”€ repository/  (Redis, cache, oracle)            â”‚
â”‚  â”œâ”€â”€ calculator/  (pool math, routing)              â”‚
â”‚  â”œâ”€â”€ service/     (business logic)                  â”‚
â”‚  â””â”€â”€ api/         (HTTP + gRPC)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“ HTTP
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Rust RPC Proxy (Shared Infrastructure)      â”‚
â”‚  â€¢ Centralized RPC management                       â”‚
â”‚  â€¢ Used by ALL services (quote, scanner, executor)  â”‚
â”‚  â€¢ Rate limiting, health monitoring                 â”‚
â”‚  â€¢ Connection pooling, circuit breaker              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Improvements

| Aspect | Before (Monolithic) | After (Clean) | Benefit |
|--------|---------------------|---------------|---------|
| **Quote Latency** | ~200ms (discovery included) | < 10ms (Redis lookup) | **20x faster** |
| **Code Size** | 50K lines | 15K lines (quote) + 8K (discovery) | **85% reduction** |
| **Test Coverage** | 20% | > 80% target | **4x better** |
| **Maintainability** | Poor (monolithic) | Excellent (clean architecture) | **High** |
| **RPC Reusability** | No (embedded) | Yes (shared proxy) | **High** |
| **Deployment Risk** | High (single service) | Low (independent services) | **Lower** |

---

## Go vs Rust Decision

### Performance Analysis: Is Rust Worth It?

**Go (Optimized):**
```
Redis pool lookup:      0.5ms
Pool math calculation:  0.2ms
Price calculation:      0.1ms
Response serialization: 0.1ms
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL:                  0.9ms âœ… Excellent
```

**Rust (Theoretical):**
```
Redis pool lookup:      0.3ms  (faster client)
Pool math calculation:  0.1ms  (zero-cost abstractions)
Price calculation:      0.05ms (SIMD)
Response serialization: 0.05ms (serde zero-copy)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL:                  0.5ms âœ… Better, but marginal
```

**Verdict: 0.4ms improvement (44% faster) is NOT worth 5 extra weeks**

### Decision Matrix

| Factor | Go | Rust | Winner |
|--------|-----|------|--------|
| **Development Speed** | 2-3 weeks âœ… | 6-8 weeks âš ï¸ | **Go** |
| **Team Knowledge** | Proven âœ… | Learning curve âš ï¸ | **Go** |
| **Performance** | <10ms âœ… | <5ms âœ… | Tie (both good enough) |
| **Code Reuse** | Can reuse router/pool âœ… | Rewrite everything âŒ | **Go** |
| **Risk** | Low âœ… | High âš ï¸ | **Go** |

**Decision: Go for Quote Service âœ…**

**Rationale:**
1. Solo developer - stick to known language
2. Time to market - 2-3 weeks vs 6-8 weeks
3. Performance - <10ms target easily met with Go
4. Code reuse - can reuse existing `pkg/router`, `pkg/pool`
5. Risk mitigation - proven technology, easy rollback

### Hybrid Approach (Best of Both Worlds)

```
Use Go for:
âœ… Quote Service (fast delivery, good enough performance)
âœ… Pool Discovery (I/O bound, Go is perfect)

Use Rust for:
âœ… RPC Proxy (shared infrastructure, worth investment)
âœ… Transaction Builder (memory-critical, zero-copy)
âœ… Shredstream Parser (ultra-low latency)
```

**Result: Fast delivery where it matters, peak performance where it counts**

---

## HTTP + gRPC: Combined vs Split

### The Question

Should HTTP and gRPC be in one service or split into two separate services?

### Option 1: Combined (RECOMMENDED âœ…)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Quote Service (Single Process)       â”‚
â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ HTTP :8080  â”‚   â”‚ gRPC :50051    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                    â”‚          â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                  â–¼                      â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚    â”‚  In-Memory Cache         â”‚        â”‚
â”‚    â”‚  (SHARED! âœ…)            â”‚        â”‚
â”‚    â”‚  0.3ms access            â”‚        â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Performance:**
- HTTP cached quote: **0.3ms** âœ…
- gRPC stream update: **0.15ms** âœ…
- Throughput: **10,000 req/s** âœ…

### Option 2: Split (NOT RECOMMENDED âš ï¸)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  HTTP Service :8080      â”‚
â”‚  Uses Redis cache        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â–¼
   Redis (1ms overhead)
        â–²
â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  gRPC Service :50051     â”‚
â”‚  Uses Redis cache        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Performance:**
- HTTP cached quote: **1.2ms** (4x slower âŒ)
- gRPC stream update: **1.05ms** (7x slower âŒ)
- Throughput: **1,000 req/s** (10x less âŒ)

### Performance Comparison

| Scenario | Combined | Split (Redis) | Difference |
|----------|----------|---------------|------------|
| **Cached Quote (HTTP)** | 0.3ms âœ… | 1.2ms âš ï¸ | **4x slower** |
| **gRPC Stream Update** | 0.15ms âœ… | 1.05ms âš ï¸ | **7x slower** |
| **Throughput** | 10K req/s âœ… | 1K req/s âš ï¸ | **10x less** |
| **Memory** | 300MB âœ… | 600MB âš ï¸ | **2x more** |
| **Services to Deploy** | 1 âœ… | 2 âš ï¸ | **2x ops** |

### Decision: COMBINED âœ…

**Why Combined Wins:**

1. **Performance** - 4-7x faster (CRITICAL for HFT)
   - In-memory cache: 0.3ms
   - Redis cache: 1.2ms
   - **Redis overhead kills performance**

2. **Throughput** - 10x higher capacity
   - Combined: 10K req/s
   - Split: 1K req/s (Redis bottleneck)

3. **Simplicity** - Solo developer
   - 1 service vs 2 services
   - 1 deployment vs 2 deployments

4. **Memory Efficiency** - 50% less RAM
   - Combined: 300MB (single in-memory cache)
   - Split: 600MB (2x Redis storage)

**The Insight:** For HFT systems targeting sub-10ms latency, **in-memory cache sharing** between HTTP and gRPC is non-negotiable. The 1ms Redis overhead destroys performance gains from service separation.

---

## Clean Architecture Benefits

### Internal Package Structure

**New Directory Layout:**

```
go/
â”œâ”€â”€ cmd/
â”‚   â”œâ”€â”€ quote-service/
â”‚   â”‚   â””â”€â”€ main.go                    # 100 lines (ONLY DI wiring)
â”‚   â””â”€â”€ pool-discovery-service/
â”‚       â””â”€â”€ main.go
â”‚
â””â”€â”€ internal/
    â”œâ”€â”€ quote-service/
    â”‚   â”œâ”€â”€ domain/                    # Core business logic
    â”‚   â”‚   â”œâ”€â”€ interfaces.go          # PoolReader, PriceCalculator
    â”‚   â”‚   â”œâ”€â”€ quote.go               # Quote, Pool models
    â”‚   â”‚   â””â”€â”€ errors.go              # Business errors
    â”‚   â”‚
    â”‚   â”œâ”€â”€ repository/                # Data access
    â”‚   â”‚   â”œâ”€â”€ pool_repository.go     # Redis pool reader
    â”‚   â”‚   â”œâ”€â”€ cache_repository.go    # In-memory cache
    â”‚   â”‚   â””â”€â”€ oracle_repository.go   # Pyth/Jupiter
    â”‚   â”‚
    â”‚   â”œâ”€â”€ calculator/                # Business logic
    â”‚   â”‚   â”œâ”€â”€ pool_calculator.go     # AMM math
    â”‚   â”‚   â”œâ”€â”€ slippage_calculator.go # Price impact
    â”‚   â”‚   â””â”€â”€ route_optimizer.go     # Best route
    â”‚   â”‚
    â”‚   â”œâ”€â”€ service/                   # Orchestration
    â”‚   â”‚   â”œâ”€â”€ quote_service.go       # Quote orchestration
    â”‚   â”‚   â”œâ”€â”€ price_service.go       # Price calculation
    â”‚   â”‚   â””â”€â”€ cache_service.go       # Cache management
    â”‚   â”‚
    â”‚   â””â”€â”€ api/                       # HTTP + gRPC
    â”‚       â”œâ”€â”€ http/handler.go        # Gin handlers
    â”‚       â””â”€â”€ grpc/server.go         # gRPC streaming
    â”‚
    â””â”€â”€ pool-discovery/
        â”œâ”€â”€ scanner/                   # DEX scanners
        â”œâ”€â”€ storage/                   # Redis writer
        â””â”€â”€ scheduler/                 # Periodic job
```

### Code Size Reduction

**Before (Monolithic):**
```
cmd/quote-service/
â”œâ”€â”€ main.go           52,844 bytes âŒ
â”œâ”€â”€ cache.go          96,419 bytes âŒ
â”œâ”€â”€ grpc_server.go    40,734 bytes âŒ
â””â”€â”€ ... 17 more files

TOTAL: 317KB (50K+ lines) âŒ
```

**After (Clean Architecture):**
```
internal/quote-service/
â”œâ”€â”€ domain/           4,500 bytes âœ…
â”œâ”€â”€ repository/       10,000 bytes âœ…
â”œâ”€â”€ calculator/       10,000 bytes âœ…
â”œâ”€â”€ service/          9,000 bytes âœ…
â””â”€â”€ api/              10,000 bytes âœ…

cmd/quote-service/
â””â”€â”€ main.go           3,000 bytes âœ…

TOTAL: 46.5KB (15K lines) âœ…

REDUCTION: 85% less code! âœ…
```

### Testability Example

**Before (Impossible to Test):**

```go
// All dependencies hard-coded
func (c *QuoteCache) UpdateQuote() {
    pools := c.solClient.QueryAllPools(...) // Hard-coded RPC âŒ
    c.wsPool.Subscribe(...)                  // Hard-coded WS âŒ
    // Cannot inject mocks, must use real infrastructure
}

// Test coverage: 20% (too hard to test)
```

**After (Easy to Test):**

```go
// All dependencies are interfaces
type QuoteService struct {
    poolRepo     domain.PoolReader      // Interface âœ…
    calculator   domain.PriceCalculator // Interface âœ…
    cacheManager domain.CacheManager    // Interface âœ…
}

// Test with mocks
func TestGetQuote(t *testing.T) {
    mockPoolRepo := &MockPoolReader{
        pools: testPools, // Inject test data
    }
    mockCalculator := &MockPriceCalculator{
        output: expectedOutput,
    }
    mockCache := &MockCacheManager{}

    service := NewQuoteService(mockPoolRepo, mockCalculator, mockCache)

    quote, err := service.GetQuote(ctx, "SOL", "USDC", 1000000000)

    assert.NoError(t, err)
    assert.Equal(t, expectedOutput, quote.OutputAmount)
}

// Test coverage: 80%+ (easy to test with mocks) âœ…
```

### Single Responsibility Principle

**Each package has ONE job:**

| Package | Responsibility | Example |
|---------|----------------|---------|
| `domain/` | Define interfaces and models | `type PoolReader interface { ... }` |
| `repository/` | Data access (Redis, cache) | `GetPoolsByPair(...)` |
| `calculator/` | Business logic (pool math) | `CalculateQuote(pool, amount)` |
| `service/` | Orchestration | `GetQuote()` - coordinates repositories + calculators |
| `api/` | HTTP + gRPC handlers | Parse request, call service, return response |

**Benefits:**
- âœ… Easy to understand (each package is small and focused)
- âœ… Easy to test (inject dependencies via interfaces)
- âœ… Easy to change (modify one package without affecting others)
- âœ… Easy to extend (add new calculators, repositories, etc.)

---

## Technology Stack Decisions

### Final Technology Stack

| Component | Technology | Rationale |
|-----------|-----------|-----------|
| **Quote Service** | **Go** | Fast delivery (2-3 weeks), proven, <10ms easily met, can reuse code |
| **Pool Discovery** | **Go** | I/O bound (RPC calls), Go perfect for concurrency |
| **RPC Proxy** | **Rust** | Shared by ALL services, worth investment, ideal for connection pooling |
| **HTTP + gRPC** | **Combined in ONE service** | Shared cache critical (4-7x faster), simpler deployment |

### Architecture Principles

1. **Clean Architecture** âœ…
   - Domain layer (interfaces + models)
   - Service layer (business logic)
   - Repository layer (data access)
   - API layer (HTTP + gRPC handlers)

2. **Service Separation** âœ…
   - Pool Discovery: Independent background job
   - Quote Service: Pure calculation + serving
   - RPC Proxy: Centralized RPC management

3. **Cache Strategy** âœ…
   - Pool metadata: Redis (slow-changing, shared)
   - Quote cache: In-memory (fast, instance-local)
   - NO shared quote cache via Redis (defeats performance)

4. **Testing Strategy** âœ…
   - Unit tests: >80% coverage (table-driven, mocks)
   - Integration tests: Real Redis, synthetic data
   - Load tests: 1000 req/s sustained

---

## Expected Improvements

### Performance Metrics

| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| **Quote Latency (cached)** | ~5ms | < 5ms | Same (already fast) |
| **Quote Latency (uncached)** | ~200ms | < 50ms | **4x faster** |
| **Throughput** | 500 req/s | 10K req/s | **20x higher** |
| **Memory Usage** | 800MB | 300MB | **63% reduction** |

### Code Quality Metrics

| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| **Lines of Code** | 50K+ | 15K | **70% reduction** |
| **Test Coverage** | ~20% | > 80% | **4x better** |
| **Largest File** | 96KB | < 10KB | **90% reduction** |
| **Package Structure** | Monolithic | Clean architecture | **Excellent** |

### Maintainability Improvements

**Before:**
- âŒ Adding a new DEX protocol: Touch 5+ files, 200+ lines
- âŒ Fixing a bug: Search through 50K lines, unpredictable side effects
- âŒ Writing tests: Requires full infrastructure (Redis, NATS, RPC)
- âŒ Understanding code: Must read entire 96KB `cache.go`

**After:**
- âœ… Adding a new DEX protocol: Implement `Protocol` interface, register in DI (50 lines)
- âœ… Fixing a bug: Isolated in one package (100-200 lines to search)
- âœ… Writing tests: Unit tests with mocks (no infrastructure)
- âœ… Understanding code: Read one package at a time (500-1000 lines max)

---

## Conclusion: Building for the Future

### Why This Matters

Building trading systems is not just about **making it work today**â€”it's about **building for tomorrow**. The difference between a successful system and a failed one often comes down to **maintainability**.

**Bad architecture compounds:**
- Year 1: "It's a bit messy, but it works"
- Year 2: "Adding features is getting harder"
- Year 3: "We can't change anything without breaking something"
- Year 4: "We need to rewrite everything" â† Too late

**Good architecture scales:**
- Year 1: "Clean architecture takes more time upfront"
- Year 2: "Adding features is still easy"
- Year 3: "We can refactor safely with 80% test coverage"
- Year 4: "The system is maintainable and growing" â† Success

### The Investment

**Time Required:** 6 weeks
- Week 1-3: Parallel development (no disruption)
- Week 4: Canary testing (10% traffic)
- Week 5: Gradual rollout (10% â†’ 100%)
- Week 6: Production hardening

**Risk:** Low (incremental, rollback-friendly)

**Outcome:** Production-ready, maintainable, performant quote service for the next 5+ years

### The Alternative

**If we don't rewrite:**
- Technical debt grows exponentially
- Adding features becomes impossible
- Bug fixes become dangerous
- Team velocity grinds to zero
- Eventually forced to rewrite under pressure (high risk)

**The choice is clear: Invest 6 weeks now, or pay 10x more later.**

### Merry Christmas! ğŸ„

As we close out 2025 and look toward 2026, I'm excited about this architectural evolution. Building robust, maintainable systems is what separates **hobby projects** from **production systems**.

**Here's to clean architecture, sustainable codebases, and successful trading in 2026!** ğŸ‰

**Wishing everyone a Merry Christmas and a Happy New Year!** May your trades be profitable and your bugs be few! ğŸš€

---

## References

- [Quote Service Rewrite Plan (docs/26-QUOTE-SERVICE-REWRITE-PLAN.md)](https://github.com/guidebee/solana-trading-system)
- [Rust RPC Proxy Design (docs/25-RUST-RPC-PROXY-DESIGN.md)](https://github.com/guidebee/solana-trading-system)
- [Clean Architecture by Robert C. Martin](https://blog.cleancoder.com/uncle-bob/2012/08/13/the-clean-architecture.html)
- [Go Project Layout Standards](https://github.com/golang-standards/project-layout)

---

**Next Post:** Quote Service Rewrite - Phase 1 Implementation (Foundation Skeleton)

**Stay tuned for the journey from architectural debt to clean, maintainable code!** ğŸ„